---
title: "R-Project"
output:
    html_document:
        toc: true
---

# Dependencies

```{r, message=FALSE}
library(tidyverse)
library(tidyquant)
library(MASS)
library(gridExtra)
library(poweRlaw)
library(lmtest)
library(dynlm)
```

# Einleitung

Im November des vergangenen Jahres kündigt der Börsengigant "S&P Dow Jones Indices" den "S&P 500 Twitter Sentiment Index" an. Dabei handelt es sich um einen Aktienindex, der Wertpapiere nach ihrer aktuellen Popularität auf Twitter bewertet. Und es ist längst nicht der einzige Fall, in dem alteingessene Finanzmarkt-Akteure Interesse an Social Media Daten für Kursvorhersagen demonstrieren. Spätestens nach den Geschenissen um "Game Stop" im Januar 2021 wurde deutlich, welche Macht Kleininvestoren im Zeitalter der globalen Vernetzung entfalten können, wenn sie sich nur koordinieren.

Im hochvolatilen Crypto-Markt ist der Effekt noch ausgeprägter, im Kurs von "Dogecoin" hinterlässt jeder Tweet von Elon Musk deutliche Spuren. Wenn ein Coin große Popularität erreicht, ist aber eigentlich schon die größte Chance für Investoren vorbei. Am 8. April 2021 lag der "Dogecoin" Kurs bei 5 Cent, genau einen Monat später bei 52 Cent. Mit dieser mehr als 10-Fachung schaffte der Coin es bekanntlich sogar in die internationale Presse. Aber ist man stattdessen drei Monate früher eingestiegen, als am 6. Januar der Kurs noch bei 0.0085€ lag, hätte man sein Geld versechzigfachen können, wenn man schon anfang 2017 beim Preis von 0.000196€ eingestiegen wäre, hätte man sein Geld um den Faktor 2600 gesteigert. Das große Geld mit Cryptowährungen macht man also als ganz früher Einsteiger, lange bevor eine Währung populär ist und das Volumen so groß ist dass Preissteigerungen um 3 Größenordnungen ökonomisch unrealistisch sind.

Die Korrelation zwischen Social-Media Aktivität und Aktienpreisen wurde bereits in einer Vielzahl von Forschungsarbeiten untersucht [1], wobei wiederholt die Schlussfolgerung erreicht wird, dass Nennungen alleine kein zuverlässiger Indikator für signifikante positive Preisbewegungen sind. Grund dafür sind die beidseitige Wechselwirkung zwischen Social-Media Aktivität und Asset-Preis sowie das Vermischen positiver und negativer Sentimente in den Beiträgen. Vereinfacht gesagt, die Social-Media Daten beinhalten zu viele Störsignale und müssen zunächst aufwendig gefiltert werden um jeglichen Nutzen für die Preisvorhersage zu liefern.

Diese Exploration wurde Motiviert durch die Hypothese, dass die Aktivität in sozialen Netzwerken bezüglich Assets von *geringer* popularität bedeutend weniger Störsignale enthält und daher, potenziell bereits in roher Form, einen besseren Indikator für Preisbewegungen liefert.

# Explorative Analyse

## Datensatz

Bei der Erhebung der Social Media Daten kam schnell das Problem auf, dass Reddits offizielle API auf 1000 Ergebnisse pro Suchanfrage limitiert ist. Jedoch existiert das inoffizielle ''Pushshift''-Archiv, das Suchen in einem präzisen Zeitrahmen ohne ein Limit für die Anzahl der Suchergebnisse ermöglicht. Aber auch da war mit einem Ratelimit von einer Anfrage pro Sekunde unsere geplante Auswertung von über 4000 Coins über einen Zeitraum von 4 Jahren nicht möglich. Also entschieden wir uns das komplette Pushshift-Archiv aller Kommentare auf Reddit herunterzuladen, dass in seinen 1.4 Terrabyte auf Academic Torrents (https://academictorrents.com/details/90e7a746b1c24e45af0940b37cffcec7c96c8096) als Peer-to-Peer-Download zur Verfügung steht. Wir beschränkten uns auf den Zeitraum vom 1. Januar 2017 bis zum 31. Dezember 2020 und ließen für jeden Tag die Anzahl der Erwähungen aller ausgewählten Cryptowährungen auswerten, was mehrere Tage Programmlaufzeit in Anspruch nahm. Um effizienter mit den Daten arbeiten zu können, sammelten wir die Erwähnungen in einer SQLite-Datenbank, bei dem der Schlüssel aus Datum und Coinname zusammengesetzt ist (bsp. ''2018-08-04-Bitcoin'').

Als zweite Datenquelle haben wir die API von Cryptocompare genutzt. Dort haben wir den täglichen Kurs aller Cryptowährungen über die vier Jahre gesammelt und in einzelnen CSV-Dateien abgespeichert. Zusätzlich haben wir noch deren Social Media Statistiken gespeichert, unter anderem "reddit-mentions", "twitter-likes" und Aufrufzahlen auf Cryptocompare. Somit besteht der Datensatz aus 3 Teilen. Die Einzelheiten der jeweiligen Datensätze werden im Folgenden kurz anhand des Beispiels *Shiba Inu* illustiert:

```{r}
symbol <- "SHIB"
```

**Preis Historien** (*data/price_histories/*)

Diese Liste an `*.csv` Dateien enthält preisbezogene Daten für `r length(dir(path="data/price_histories", pattern="\\.csv$")) - 1` Cryptowährungen über den Zeitraum vom 01.01.2017 bis zum 31.12.2021. Dabei werden pro Tag die folgenden Datenpunkte erfasst:

- `open`, `high`, `low`, `close`, welche den ersten, höchsten, tiefsten und letzten Preis des jeweiligen Tages in USD angeben
- `volumeto`, `volumefrom`, welche das Käuf- und Verkaufvolumen in USD angeben
- `conversionType`, `conversionSymbol`, die angeben falls der Preis von einer anderen Basiswährung bezogen und lediglich in USD umgerechnet wurde

```{r, fig.height=3, message=FALSE, warning=FALSE}
price_df <- as_tibble(read.csv(file=sprintf("data/price_histories/%s.csv", symbol))) %>%
    mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
    filter(open > 0 | close > 0 | high > 0 | low > 0)

price_df %>%
    ggplot(aes(x=time, y=close)) +
    geom_candlestick(aes(open=open, high=high, low=low, close=close)) +
    labs(title=symbol, x="Time", y="Price (USD)")
```

```{r, fig.height=2, message=FALSE, warning=FALSE}
price_df %>%
    ggplot(aes(x=time, y=volumeto)) +
    geom_line() +
    labs(title=symbol, x="Time", y="Volume (USD)")
```

**Marktkapitalisierungen** (*data/mktcap.csv*)

Diese `*.csv` Datei enthält die Marktkapitalisierung (`mktcap`) für 5941 Cryptowährungen, bezogen am 28.01.2022. Außerdem enthalten sind die gesamtmenge an Tokens (`supply`), sowie die gesamtmenge im Umlauf (`circulatingSupply`).

```{r}
mktcap_df <- as_tibble(read.csv(file="data/mktcap.csv"))
summary(mktcap_df %>% dplyr::select(-X, -conversionType, -conversionSymbol))
```

**Social-Media Statistiken** (*data/social_stats/*)

Die `*.csv` Dateien im *social_stats* Ordner enthalten Social-Media Statistiken für `r length(dir(path="data/social_stats", pattern="\\.csv"))` Cryptoassets. Dabei wird je Asset eine vielzahl an Variablen geführt von denen wir uns hier auf `reddit_comments_per_day`, `reddit_posts_per_day`, und `total_page_views` beschänken.

Wie die Namen bereits vermuten lassen geben die ersten beiden dieser Variablen die Anzahl der Reddit-Kommentare bzw. Posts pro Tag für eine bestimmte Cryptowährung. Die letze Variable gibt die Anzahl an Seitenaufrufen auf der Domain *www.cryptocompare.com* die in jeglichem Zusammenhang mit dem jeweiligen Cryptoasset stehen.

```{r}
plot_social <- function(symbol) {
    social_df <- as_tibble(read.csv(file=sprintf("data/social_stats/%s.csv", symbol))) %>%
        mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
        mutate(total_page_views=total_page_views-lag(total_page_views)) %>%
        dplyr::select(time, reddit_comments_per_day, reddit_posts_per_day, total_page_views) %>%
        filter(reddit_comments_per_day > 0 | reddit_posts_per_day > 0 | total_page_views > 0)

    plot_cc_social <- social_df %>%
        ggplot(aes(x=time, y=total_page_views)) +
        geom_line() +
        labs(title=sprintf("%s: CryptoCompare Total Views", symbol),
             x="Time",
             y="#Views")

    plot_reddit_post <- social_df %>%
        ggplot(aes(x=time, y=reddit_posts_per_day)) +
        geom_line(color="darkorange3") +
        labs(title=sprintf("%s: Reddit Posts", symbol),
             x="Time",
             y="#Posts")

    plot_reddit_comment <- social_df %>%
        ggplot(aes(x=time, y=reddit_comments_per_day)) +
        geom_line(color="orange") +
        labs(title=sprintf("%s: Reddit Comments", symbol),
             x="Time",
             y="#Comments")
    
    grid.arrange(
        plot_cc_social,
        plot_reddit_post,
        plot_reddit_comment,
        nrow=3)
}
```

```{r}
plot_social(symbol)
```


## Filterkriterien

Im Hinblick auf unsere Forschungsfrage interessieren uns nur bestimmte Abschnitte der uns vorliegenden Zeitserien; nämlich die Abschnitte bis kurz nachdem die jeweilige Cryptowährung ein *breakout* erlebt hat.

Der Einfachheit halber wird angenommen, dass ein Token nur einmal *prominent* werden kann. Daher wird je Asset maximal ein Zeitpunk gesetzt, wo der erste *breakout* angenommen wird.

Zur Klassifizierung werden hier die Faktoren: Preis, Marktkapitalisierung, und Popularität berücksichtigt. Die Klassifikationskriterien werden im Folgenden illustriert.

### Preis

Von Bedeutung sind signifikante positive Preisbewegungen innerhalb kurzer Zeit. Als Indikator wird daher die Differenz zwischen Preis und einem 50-Tägigen laufenden Mittel (SMA50) betrachtet.

```{r, fig.height=6, message=FALSE, warning=FALSE}
# price plot for reference
plot_price <- price_df %>%
    ggplot(aes(x=time, y=close)) +
    geom_candlestick(aes(open=open, high=high, low=low, close=close)) +
    geom_ma(ma_fun = SMA, n = 50, color = "black", size = 1) +
    geom_smooth(se=FALSE) +
    labs(title=symbol, x="Time", y="Price (USD)")

# simple moving average
sma <- function(x, n=50) {
    len = length(x)
    if(len >= n) {
        return(stats::filter(x, rep(1/n, n), sides = 1))
    } else {
        return(rep(NA, n))
    }
}

# relative delta indicator
relative_delta <- function(x, n=50) {
    len = length(x)
    ma <- sma(x,n)
    relative_delta = list()
    for (i in 1:len) {
        relative_delta[i] <- if (is.na(ma[i])) NA else (x[i] - ma[i])/ma[i]
    }
    return(unlist(relative_delta))
}

plot_relative_delta <- ggplot(data.frame(x=price_df$time, y=relative_delta(price_df$close))) +
    geom_area(mapping=aes(x=x, y=y, fill=(y > 0))) +
    geom_line(aes(x=x, y=y)) +
    geom_hline(yintercept=2, linetype='dotted') +
    geom_hline(yintercept=1, linetype='dotted') +
    geom_hline(yintercept=-1, linetype='dotted') +
    geom_hline(yintercept=-2, linetype='dotted') +
    guides(fill="none") +
    labs(title=sprintf("%s: Relative Delta", symbol), x="Time", y="y")

grid.arrange(plot_price, plot_relative_delta, nrow=2, heights=c(3,3))
```

```{r, include=FALSE}
# volume
price_df %>%
    ggplot(aes(x=time, y=volumeto)) +
    geom_segment(aes(xend=time, yend=0)) +
    geom_ma(ma_fun = SMA, n = 50, color = "black", size = 1) +
    geom_smooth(se=FALSE)

```

Es wird festgelegt, dass eine Preissteigerung von mehr als 200% bezüglich des Durschnittspreises der letzten 50 Tage eine singifikante positive Preisbewegung klassifiziert.

**Kriterium**: `relative_delta > 2.0`

### Marktkapitalisierung

Da uns nur die aktuellen und keine historischen Daten für die Marktkapitalisierung der hier betrachteten Assets vorliegen wird die historische Marktkapitalisierung mithilfe der Preishistorie abgeschätzt, vorgemerkt, unter Vernachlässigung von quantitativem Easing/Tightening.

```{r, fig.height=3, message=FALSE, warning=FALSE}

# cleanup
mktcap_df <- mktcap_df %>%
    filter(supply > 0 &
           (circulatingSupply > 0
            | conversionType == "direct")) %>%
    arrange(mktcap)

r_mktcap <-  mktcap_df %>% pull(mktcap) %>% log1p()

mktcap_df %>%
    ggplot() +
    geom_hline(yintercept=mean(r_mktcap), color="grey") +
    geom_point(aes(x=1:length(r_mktcap), y=log1p(circulatingSupplyMktcap)),
               color="grey") +
    geom_point(aes(x=1:length(r_mktcap), y=log1p(mktcap))) +
    geom_vline(xintercept=round(0.90*length(r_mktcap)), color="blue") +
    xlab("Ordnungsstatistik")

plot_q <- mktcap_df %>%
    ggplot(aes(sample=log1p(mktcap))) +
    geom_qq() +
    geom_qq_line() +
    labs(title="Q-Q-Normal")

norm_fit <- fitdistr(r_mktcap, "normal")
plot_d <- mktcap_df %>%
    ggplot(aes(x=log1p(mktcap))) +
    stat_function(fun=dnorm,
                  args=list(mean=norm_fit$estimate["mean"], 
                            sd=norm_fit$estimate["sd"]),
                  color="red", size=0.25) +
    geom_density() +
    labs(title="Density")

grid.arrange(plot_q, plot_d, ncol=2)
```

Interessanterweise wird `log1p(mktcap)` grob von einer Normalverteilung approximiert (hier mit Erwartungswert `r norm_fit$estimate["mean"]` und Standardabweichung `r norm_fit$estimate["sd"]`). Inspektion mit einem Tail-Fitter liefert:

```{r, message=FALSE, warning=FALSE}
model_pl <- conlnorm$new(mktcap_df %>% pull(mktcap))
est_xmin <- estimate_xmin(model_pl)
model_pl$setXmin(est_xmin)
plot(model_pl)
lines(model_pl, col=2)
print(mktcap_df %>% filter(mktcap > est_xmin$xmin) %>% nrow / mktcap_df %>% nrow)
```

Das Modell erreicht immer noch einen *hinreichend guten Fit* (minimieren der Kolmogorow-Smirnow-Statistik) wenn nur die 3% der Assets mit der niedrigsten Marktkapitalisierung ausgeschlossen werden und ist damit eine sehr gute Approximation.

Diese Beobachtung regt dazu an in der Popularität von Cryptowährungen eine ähnliche Verteilung wie besipielsweise eine Potenzverteilung zu vermuten. Erfahrungsgemäß folgt der Bekanntheitsgrad Angehöriger einer Kategorie einer Potenz-artigen Verteilung wobei ein kleiner Prozentteil nahezu die gesamte Popularität beansprucht. [2] Dies motiviert folgendes Filterkriterium:

Für die Analyse werden alle Assets aussortiert, deren Marktkapitalisierung unmittelbar vor dem Zeitpunkt des *breakouts* nicht innerhalb des linksseitigen 90%-Quantils lag. Assets deren Marktkapitaldaten nicht im Datensatz enthalten sind, werden auch hinzugefügt, da bei diesem Datensatz tendeziell die größten Assets am vollständigsten enthalten sind.

**Krierium**: `mktcap_prob() ?? 0.0 < 0.9`

### Popularität

Die Popularität wird anhand der Reddit-Aktivität sowie der Anzahl zugehöriger Anfragen auf der Seite *www.cryptocompare.com* bemessen.

Um zu beobachten, in welchen charakteristischen Merkmalen sich die hier betrachteten Daten für Tokens von unterschiedlicher popularität unterscheiden, vergleichen wir zunächst die Crypto-Tokens *Air* (unpopulär), *Shiba Inu* (mittel-populär) und *Bitcoin* (populär).

```{r, fig.height=4, message=FALSE, warning=FALSE, eval=TRUE}
plot_social("AIR")
plot_social(symbol) # SHIB
plot_social("BTC")
```

Während die Social-Media Daten von BTC und SHIB sich in einer ähnlichen Größenordnung bewegen ist die Social-Media Aktivität von AIR 1-2 Größenordnungen kleiner.

Wir stellen aber fest, dass diese Daten in roher Form kein absolutes Maß für die Popularität des jeweiligen Assets darstellen und daher wenn nur nach sorgfältiger Analyse eine zuverlässige Aussage über die absolute Popularität treffen. Dies ist vor allem Fehleranfällig. Da wir mit der Marktkapitalisierung bereits ein gutes Kriterium haben um bereits populäre Assets herauszufiltern werden wir davon absehen diese Daten als Filterkriterium für Breakouts zu verwenden.

**Datensatz**

Da Tidyverse Dataframes auf maximal 2 Dimensionen beschränkt sind wird der vollständige Datensatz nicht explizit erstellt. Statdessen werden die Daten am Ende des Reports, inkremental, Währung für Währung, aggregiert, gefilter und nach Durchführung der jeweiligen Tests wieder verworfen.

## Exploration

Nachdem die Kriterien zur Klassifizierung von *price-breakouts* festgelegt wurden vergleichen wir zunächst in einer Übersicht die unterschiedlichen Variablen von verschiedenen Tokens um die Plausibilität der Forschungsfrage zunächst einzuschätzen und gegebenenfalls noch weitere Zusammenhäge zu erkennen.

```{r, fig.height=7, message=FALSE, warning=TRUE}

plot_full_profile <- function(symbol) {
    # load cleaned price data
    price_df <- as_tibble(read.csv(file=sprintf("data/price_histories/%s.csv", symbol))) %>%
        mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
        mutate(close=ifelse(relative_delta(.$close) >= 10, runmed(close, 49), close)) %>%
        mutate(open=ifelse(relative_delta(.$open) >= 10, runmed(open, 49), open)) %>%
        mutate(high=ifelse(relative_delta(.$high) >= 10, runmed(high, 49), high)) %>%
        mutate(low=ifelse(relative_delta(.$low) >= 10, runmed(low, 49), low)) %>%
        filter(open > 0 | close > 0 | high > 0 | low > 0)

    start_date <- (price_df %>% pull(time))[1]

    plot_price <- price_df %>%
        ggplot(aes(x=time, y=close)) +
        geom_line(color="navy") +
        labs(title=symbol, x="Time", y="Price (USD)")
    
    plot_volume <- price_df %>%
        ggplot() +
        geom_line(aes(x=time, y=volumeto), color="blue") +
        #geom_line(aes(x=time, y=volumefrom), color="grey") +
        labs(title=sprintf("%s: Volume", symbol), x="Time", y="Volume (USD)")

    # load social data
    social_df <- as_tibble(read.csv(file=sprintf("data/social_stats/%s.csv", symbol))) %>%
        mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
        mutate(total_page_views=total_page_views-lag(total_page_views)) %>%
        dplyr::select(time, reddit_comments_per_day, reddit_posts_per_day, total_page_views) %>%
        filter(time >= start_date)

    plot_cc_social <- social_df %>%
        ggplot(aes(x=time, y=total_page_views)) +
        geom_line() +
        labs(title=sprintf("%s: CryptoCompare Total Views", symbol), x="Time", y="#Views")

    plot_reddit_post <- social_df %>%
        ggplot(aes(x=time, y=reddit_posts_per_day)) +
        geom_line(color="darkorange3") +
        labs(title=sprintf("%s: Reddit Posts", symbol), x="Time", y="#Posts")

    plot_reddit_comment <- social_df %>%
        ggplot(aes(x=time, y=reddit_comments_per_day)) +
        geom_line(color="orange") +
        labs(title=sprintf("%s: Reddit Comments", symbol), x="Time", y="#Comments")
    
    grid.arrange(plot_price, plot_volume, plot_cc_social, plot_reddit_post, plot_reddit_comment, nrow=5)
}
```

Im folgenden werden unter anderem erneut die Daten drei Cryptoassets *Air*, *Shiba Inu* und *Bitcoin* visualisiert, sowie *DOGE*, *MANA*, und *SAND*; jetzt allerdings zusammen mit Preis- und Volumendaten:

```{r, fig.height=7, message=FALSE, warning=FALSE, eval=TRUE}

plot_full_profile("AIR")
plot_full_profile("SAND")
plot_full_profile("MANA")
plot_full_profile(symbol) # SHIB
plot_full_profile("DOGE")
plot_full_profile("BTC")
```

Wie erwartet lassen die Plots einen Zusammenhang zwischen den Social-Media- und Preis-Daten vermuten.

Leider fällt aber auf, dass vor allem bei kleineren Cryptowährungen die Reddit Daten nur sehr unvollständig und in der Regel erst ab einem recht späten Zeitpunkt vorhanden sind. Wenn vorhanden, scheinen diese allerdings relativ gut mit den Seitenaufrufen bei CryptoCompare zu korrelieren. Da diese auch einen hinreichend guten Indikator für die Popularität einer Währung liefern sollten und vor allem aber nahezu lückenlos vorhanden sind verwerfen wir die Reddit Daten und verwenden stattdessen die CryptoCompare Seitenaufrufe als Metrik für Social-Media Aktivität.

# Methoden

## Granger Kausalität

Um zu entscheiden, ob die hier vorliegenden Social-Media Daten ein angemessener Prädiktor für den Preis des zugehörigen Tokens sind ($X \implies Y$), führen wir einen *Granger Causality* Hypothesentest zum Signifikanzniveau $\alpha = 0.05$ durch.

Bei einem Grangertest für zwei Variablen $Y\sim X$ werden standardmäßig zwei lineare Regressionsmodelle betrachtet:

$$\text{Model 1:}\quad \hat y_t = \gamma_1 + \sum_{i=1}^{n} \alpha_{1,i} y_{t-i} + \sum_{i=1}^{n} \beta_i x_{t-i} + \varepsilon_{x,t}$$
$$\text{Model 2:}\quad \tilde y_t = \gamma_2 + \sum_{i=1}^{n} \alpha_{2,i} y_{t-i} + \varepsilon_{x,t}$$

Es soll überprüft werden, ob $\text{Model 1}$ $y_t$ besser vorhersagt als $\text{Model 2}$. In einem Test werden also die folgenden Hypothesen geprüft:

$$H_0: (\beta_1, ..., \beta_n) = 0 = \beta_0, \quad \text{(Assetpreis nicht von Social-Media Daten Granger-verursacht)}$$

gegen

$$H_1: \beta_1 \neq 0 \lor ... \lor \beta_n \neq 0, \quad\text{(Social-Media Daten Granger-verursachen Assetpreis)}$$

Um diese Hypothesen zu überprüfen wird bei einem Grangertest in der Regel ein Speizialfall des sogenannter Wald-Tests[3] durchgeführt, nämlich ein globaler $F$-Test[4]. Dieser trifft eine Aussage darüber, ob mindestens eine Variable eines linearen Regressionsmodells eine signifikante Bedeutung für das Modell hat. Für $\beta = (\beta_1, ..., \beta_n)$, $\tilde y = (\tilde y_1, ..., \tilde y_n)$ und $\hat y = (\hat y_1, ..., \hat y_n)$ die von Modell 2 und 1 generierten Vorhersagen, $l$ die Anzahl der $(y_i, x_i)$ paare im Datensatz und $R^2$ das Bestimmtheitsmaß hat dieser die Teststatistik:

$$T = = \frac{\frac{\text{RSS}_1 - \text{RSS}_2}{p_1-p_2}}{\frac{\text{RSS}_2}{l-p_2}} = \frac{\sum_{i=1}^{n} (y_i - \hat y_i)^2 - \sum_{i=1}^{n} (y_i - \tilde y_i)^2}{\sum_{i=1}^{n} (y_i - \tilde y_i)^2} \frac{l-n}{n} \sim \text{F}_{n,l-n} \quad\text{bed. }H_0\text{ gilt}$$

<!-- \frac{\sum_{i=1}^{n} (\hat y_i - \overline{\hat y})^2}{\sum_{i=1}^{n} (y_i - \tilde y_i)^2} \frac{l-n}{n} -->

Liegt der $p$-Wert unterhalb des vorgegebenen Singnifikanzniveaus, so wird die Nullhypothese abgelehnt. Andernfalls wird die Alternativhypothese verworfen.

Um zu verhindern, dass die beobachteten Ergebnisse das Resultat einer beidseitigen Wechselwirkung sind wird im Anschluss ein umgekehrter Granger-Causality Test für $Y\implies X$ durchgeführt. Nur falls hier der $p$-Wert hinreichend groß ist, sodass die Hypothese, $Y$ sei ein Prädiktor für $X$, abgelehnt wird, wird der Test als erfolgreich gewertet.

# Test

Im Folgenden sehen wir ein paar Beispiele für Granger-Tests.

```{r}
granger_test <- function(x, y, order) {
    x <- ts(x)
    y <- ts(y)
    m1 <- dynlm(y ~ L(y, 1:order) + L(x, 1:order))
    m2 <- dynlm(y ~ L(y, 1:order))
    return(result <-anova(m1, m2, test="F"))
}

granger_test_full <- function(x, y, order=3) {
    test_main <- granger_test(x=x, y=y, order=order) # y ~ x
    test_reverse <- granger_test(x=y, y=x, order=order) # x ~ y
    return(list(main=test_main, reverse=test_reverse))
}

print_granger_test <- function(symbol, order=3) {
    test_df <- as_tibble(read.csv(file=sprintf("data/price_histories/%s.csv", symbol))) %>%
        mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
        mutate(close=ifelse(relative_delta(.$close) >= 10, runmed(close, 49), close)) %>%
        mutate(open=ifelse(relative_delta(.$open) >= 10, runmed(open, 49), open)) %>%
        mutate(high=ifelse(relative_delta(.$high) >= 10, runmed(high, 49), high)) %>%
        mutate(low=ifelse(relative_delta(.$low) >= 10, runmed(low, 49), low)) %>%
        filter(open > 0 | close > 0 | high > 0 | low > 0)
    
    start_date <- (test_df %>% pull(time))[1]

    test_df <- test_df %>%
        add_column(as_tibble(read.csv(file=sprintf("data/social_stats/%s.csv", symbol))) %>%
            mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
            mutate(total_page_views=total_page_views-lag(total_page_views)) %>%
            filter(time >= start_date) %>%
            dplyr::select(-time, -X))

    # show test results
    print(sprintf("Price ~ CryptoCompare Total Page Views | (%i)", nrow(test_df)))
    test <- granger_test_full(x=test_df %>% pull(close),
                         y=test_df %>% pull(total_page_views),
                         order=order)
    print(test$main)
    print("Reverse:")
    print(test$reverse)
}
```

```{r, eval=TRUE}
print_granger_test("SHIB")
print_granger_test(symbol) # SHIB
print_granger_test("BTC")
```

Hier können mehrere Beobachtungen gemacht werden:

1. BTC, das populärste der 3 Assets, liefert einen p-Wert, weit über dem gesetzten Signifikanzniveau, sodass $H_1$ abgelehnt wird, also laut diesem Test keine Kausalität zwischen den Variablen besteht.
2. Sowohl AIR, als auch SHIB haben einen p-Wert < $\alpha$. Demnach wird die Nullhypothese abgelehnt. Allerdings schlägt auch der reverse-Test an, was indikativ für eine beidseitige Wechselwirkung ist.

## Auswertung

Datensatz mit historischer Marktkapitalisierung.

```{r, warning=FALSE, cache=TRUE, eval=TRUE}
mktcap_history_df <- as_tibble(read.csv(file="data/price_histories/BTC.csv")) %>%
    mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
    dplyr::select(time)

for (mktcap_symbol in mktcap_df %>% pull(symbol)) {
    if (file.exists(sprintf("data/price_histories/%s.csv", mktcap_symbol))) {
        price <- as_tibble(
                read.csv(file=sprintf("data/price_histories/%s.csv", mktcap_symbol))) %>%
                pull(close)
        supply <- (mktcap_df %>% filter(symbol==mktcap_symbol) %>% pull(supply))[1]
        mktcap_history_df <- mktcap_history_df %>%
            add_column("{mktcap_symbol}":=price*supply)
    }
}

mktcap_history_df[mktcap_history_df == 0] <- NA # ignore mktcap = 0 for quantile
quantiles <- mktcap_history_df %>%
    dplyr::select(-time) %>%
    t() %>% as_tibble(rownames="symbol") %>% column_to_rownames("symbol") %>%
    sapply(quantile, probs=0.90, na.rm=TRUE)
mktcap_history_df <- mktcap_history_df %>% add_column("q90"=quantiles)
mktcap_history_df[is.na(mktcap_history_df)] <- 0.0
```

Die Daten werden nach den im vorigen Abschnitt spezifizierten Kriterien gefiltert. Für die gefilterten Daten wird im Anschluss je Asset ein Granger Test durchgeführt.

```{r, warning=FALSE, cache=TRUE, eval=TRUE}
file_names_price <- dir(path="data/price_histories", pattern="\\.csv$")
file_names_price <- file_names_price[-which(file_names_price == "symbols_skipped_price.csv")]
assets_no_peak <- list()
assets_ts_l50 <- list()
assets_mktcap_90 <- list()
assets_no_social <- list()

# dataframe for test results
result_df <- tibble(
    symbol = character(),
    startDate = as.Date(character()),
    endDate = as.Date(character()),
    length = numeric(),
    p = numeric(),
    F = numeric(),
    revP = numeric(),
    revF = numeric(),
    mktcapPreBr = numeric(),
    mktcapPostBr = numeric(),
    priceMin = numeric(),
    priceMax = numeric()
)

to_index <- 200#length(file_names_price)

for (name in file_names_price[1:to_index]) {
    symbol <- name %>% substr(1, nchar(name)-4)
    
    # load cleaned asset price
    price_df <- as_tibble(read.csv(file=sprintf("data/price_histories/%s.csv", symbol))) %>%
        mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
        mutate(close=ifelse(relative_delta(.$close) >= 10, runmed(close, 49), close)) %>%
        mutate(open=ifelse(relative_delta(.$open) >= 10, runmed(open, 49), open)) %>%
        mutate(high=ifelse(relative_delta(.$high) >= 10, runmed(high, 49), high)) %>%
        mutate(low=ifelse(relative_delta(.$low) >= 10, runmed(low, 49), low)) %>%
        filter(open > 0 | close > 0 | high > 0 | low > 0)

    # search for price breakouts
    breakout_times <- if(nrow(price_df) == 0) list() else price_df %>%
        filter(relative_delta(close) >= 2.0) %>% pull(time)
    if (length(breakout_times) > 0) {
        first_breakout <- breakout_times[1]
        date_start <- head(price_df,1)$time
        date_end <- min(tail(price_df,1)$time, first_breakout-28)

        # indicators require ts length >50
        if (date_end-date_start > 50) {
            # read mktcap info for first breakout
            pre_breakout_mktcap <- mktcap_history_df %>% filter(time == max(
                date_start, first_breakout-21))
            has_mktcap_data <- symbol %in% names(pre_breakout_mktcap)
            pre_breakout_mktcap_symbol <- ifelse(has_mktcap_data,
                pre_breakout_mktcap[,symbol][1,], 0.0) %>% unlist
            if (pre_breakout_mktcap_symbol <= pre_breakout_mktcap$q90) {
                # select timespan until breakout + padding
                pre_breakout_price_history <- price_df %>% filter(time <= date_end)
                
                # check if social data is available
                if (file.exists(sprintf("data/social_stats/%s.csv", symbol))) {
                    pre_breakout_social_stats <- as_tibble(
                            read.csv(file=sprintf("data/social_stats/%s.csv", symbol))) %>%
                        mutate(time=as.Date(as.POSIXct(time, origin="1970-01-01"))) %>%
                        mutate(total_page_views=total_page_views-lag(total_page_views)) %>%
                        dplyr::select(time,
                                    reddit_comments_per_day,
                                    reddit_posts_per_day,
                                    total_page_views) %>%
                        filter((reddit_comments_per_day > 0 
                                | reddit_posts_per_day > 0
                                | total_page_views > 0)
                                & time <= date_end)
                    if (nrow(pre_breakout_social_stats) > 0) {
                        # merge datasets to match in length
                        test_df <- pre_breakout_price_history %>%
                            full_join(pre_breakout_social_stats, by="time")
                        test_df <- test_df %>%
                            map_if(is.numeric, ~ifelse(is.na(.x), 0, .x)) %>%
                            as_tibble
                        
                        # run granger test
                        test <- granger_test_full(
                            pull(test_df, total_page_views),
                            pull(test_df, volumeto))
                        
                        # save results
                        post_breakout_mktcap <- mktcap_history_df %>% filter(time == date_end)
                        post_breakout_mktcap_symbol <- ifelse(has_mktcap_data,
                            post_breakout_mktcap[,symbol], 0.0) %>% unlist
                        
                        result_df <- result_df %>% add_row(symbol=symbol,
                            startDate=date_start,
                            endDate=date_end,
                            length=as.numeric(date_end - date_start),
                            p=test$main$`Pr(>F)`[2],
                            F=test$main$F[2],
                            revP=test$reverse$`Pr(>F)`[2],
                            revF=test$reverse$F[2],
                            mktcapPreBr=pre_breakout_mktcap_symbol,
                            mktcapPostBr=post_breakout_mktcap_symbol,
                            priceMin=min(pre_breakout_price_history$close),
                            priceMax=max(pre_breakout_price_history$close))
                    } else {
                        assets_no_social <- assets_no_social %>% append(list(symbol))
                    }
                } else {
                    assets_no_social <- assets_no_social %>% append(list(symbol))
                }
            } else {
                assets_mktcap_90 <- assets_mktcap_90 %>% append(list(symbol))
            }
        } else {
            assets_ts_l50 <- assets_ts_l50 %>% append(list(symbol))
        }
    } else {
        assets_no_peak <- assets_no_peak %>% append(list(symbol))
    }
}

print(unlist(assets_no_peak))
print(unlist(assets_mktcap_90))
print(unlist(assets_no_social))

result_df
```

```{r, fig.height=8, warning=FALSE, eval=TRUE}
plot_granger <- result_df %>% 
    arrange(p) %>% 
    ggplot() +
    geom_point(aes(x=1:nrow(result_df), y=p), color="black") +
    geom_point(aes(x=1:nrow(result_df), y=revP, color=p<revP)) +
    geom_linerange(aes(x=1:nrow(result_df), ymin=p, ymax=revP, color=p<revP)) +
    theme(axis.ticks.x = element_blank(),
          axis.text.x = element_blank())

plot_granger_cum <- result_df %>% ggplot() +
    geom_boxplot(aes(x=rep("Granger", nrow(result_df)), y=p)) +
    geom_boxplot(aes(x=rep("Reverse Granger", nrow(result_df)), y=revP))

grid.arrange(plot_granger, plot_granger_cum, nrow=2, heights=c(1,2))
```

```{r}
a <- 0.05
summary(result_df %>% filter(p < a, revP > a))
summary(result_df %>% filter(p > a, revP < a))
```

## Ergebnis

Der Granger Kausalitäts-Test belegt deutlich, dass eine Korrelation zwischen der Popularität eines Coins bei Reddit und seinem Trading-Volumen existiert. Da jedoch fast immer auch der Reverse Granger Kausalitäts-Test anschlägt konnte nicht festgestellt werden, ob die Popularität stärker auf das Handelsvolumen Einfluss nimmt als das Handelsvolumen auf die Popularität. Um tatsächlich sinnvoll als Grundlage für eine Investment-Strategie zu dienen müsste sichergestellt werden, dass ein plötzlicher Anstieg in der Popularität ausreichend verlässlich eine Preisexplosion mehrere Stunden im Vorraus vorhersagen kann. Denn anders als bei großen Mainstream Coins auf nutzerfreundlichen Apps ist der Erwerb von sehr unbekannten Coins meist ein sehr umständlicher Prozess.

## Bias

Es ist definitiv ein ''Survivorship bias'' zu erwarten, da auch um auf den ausführlichsten Preistrackern wie CoinMarketCap gelistet zu werden, eine gewisse Popularität nötig ist. Obwohl wir also durch die Auswahl von über 4000 Coins den Effekt so gut es geht minimieren, fallen uns auf jeden Fall sehr kleine Währungen durchs Netz. Und vor allem die Datenerhebung der Reddit Popularität ist nicht fehlerfrei. Es werden die Anzahl der Kommentare und Posts gezählt, die den Coin-Namen als Keyword enthalten. Diese Methodik ist einfach, aber auch sehr anfällig gegen Spam und andere künstlichen Verfälschungen. Genauso hat die reine Keyword-Suche erhebliche Schwächen, in den rohen Daten ist die populärste Währung nicht wie zu erwarten ''Bitcoin'' sondern die kaum getradete Währung ''Crypto''.


# Literatur

[1] Bing Li, Keith C.C. Chan, Carol Ou, Sun Ruifeng (2017). Discovering public sentiment in social media for predicting stock movement of publicly listed companies, Social Science Computer Review, https://doi.org/10.1177%2F0894439312448037.

[2] https://michaeltauberg.medium.com/power-law-in-popular-media-7d7efef3fb7c

[3] https://clas.ucdenver.edu/marcelo-perraillon/sites/default/files/attached-files/week_4_slr_ii_0.pdf

[4] https://de.wikipedia.org/wiki/Globaler_F-Test